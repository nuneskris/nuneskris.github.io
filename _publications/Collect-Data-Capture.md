---
title: "Collect: Data Capture"
collection: publications
permalink: /publication/CollectDataProfiling
excerpt: 'CDC'
date: 2024-05-01
venue: 'Processing'
tags:
  - Collect
---

For the initial load we would extract all data. This will be a one-time process where all historical data is pulled into the target system. 
If the initial load is too large, break it into smaller batches to avoid overloading the source system.

> Implementation Tip: Very often the inital load is not a simple query on the enitre table because we will hit into size contraints either because of data query tools hit a limit, transfer contraints or downstream systems are not able to read large files. We would need to break these large captures into smaller batches. These batches need to be tested early. Additionally, we need the capability to rebase the entire dataset from the source when issues with the data arise.

However in this page we will discuss the ways to capture source data changes and depending on the scenario we would need to choose the best option.

# Leveraging Timestamp Audit Columns on the database
Modern source applications include timestamp audit columns that store the date and time for every row that is either created or updated. These columns are internally managed. This is the first capability of the source system we need to analyze when designing the component to capture data.

## Analyze Timestamp Audit Columns
Analyze each table to ensure the timestamp audit column (created_at, updated_at, deleted_at) are accurate and reliable. 
There can be issues if these columns are generated by custom implementations.
Verify that all timezone consistency on all the timestamp columns to avoid inconsistencies during extraction.

## Handling Deletes
* If the source system uses soft deletes (marking a row as deleted without physically removing it), ensure the deleted_at column is used to capture these records. This is the least complex in handling deletes.
* If the source system performs hard deletes (physically removing records), we would not have an audit column to indicate deletes, we would need to handle it separately in one of the scenarios below.

## Watermarking
We need implement a watermark where the last successfully extracted timestamp is recorded. This will be used as the starting point for the next extraction. There needs to be a way this can be provided as an input for the queries to define the batch window.

### Cut-off Times
We need to ensure that the extraction process considers transaction boundaries. In most cases, we find that these tables are updated by a single transaction, which is what is desired. If there is a transaction that updates more than one table, all the tables would have the same timestamp. However, while this level of detail might seem excessive, it is important to define the cut-off time for the entire batch up to the last complete transaction to avoid partial data. These ***Batch Windows*** need to specify the ***start and end times*** for each incremental extraction. Folks only configure the start time. I have seen a situation where the source application teams did a select statement based on the current time stamp for indivual querires which created partial records. Simillarly I was asked to analyze anothe situation where duplicate records were created and it was because there was a race condition where records came through  into tables which were queued for extraction while waiting for previous table to complete. 

<img width="612" alt="image" src="/images/publications/CDCTimeStamp.png">

# No timestamp column - We would need to compare the rows.

This is a resource intensive approach where we build a full difference compare query between the data which is in the application system and data which is in the analytics system. Use this option **only** when we do not have other otions.

## Comparison Logic
* Ensure every table has a primary key or a unique identifier to accurately identify records. There was a case where we Computed hash values for the significant columns we were interested in and used that to easily compare data between snapshots. The logic is simple and similar to the below logic.

 ```sql
-- Current state of the data
WITH current_data AS (
    SELECT id, col1, col2
    FROM source_table
),
-- Previous state of the data (last snapshot)
previous_data AS (
    SELECT id, col1, col2
    FROM staging_table
),
-- Identify new records
new_records AS (
    SELECT c.id, c.col1, c.col2
    FROM current_data c
    LEFT JOIN previous_data p ON c.id = p.id
    WHERE p.id IS NULL
),
-- Identify updated records
updated_records AS (
    SELECT c.id, c.col1, c.col2
    FROM current_data c
    JOIN previous_data p ON c.id = p.id
    WHERE c.col1 != p.col1 OR c.col2 != p.col2
),
-- Identify deleted records
deleted_records AS (
    SELECT p.id, p.col1, p.col2
    FROM previous_data p
    LEFT JOIN current_data c ON p.id = c.id
    WHERE c.id IS NULL
)
-- Combine all changes
SELECT * FROM new_records
UNION ALL
SELECT * FROM updated_records
UNION ALL
SELECT * FROM deleted_records;
```
### Very import recommendations
* Use ETL tools or scripts if possible. Dont build this. These typically support legacy applications and the solutions which we would build may only be temporary.
* The AMS Team needs to continuously monitor the process to ensure it is working correctly and efficiently. Establish month audits and have validation queries to ensure counts and values are correct.
* Document the entire process, including the logic used for diff queries and any assumptions made.

