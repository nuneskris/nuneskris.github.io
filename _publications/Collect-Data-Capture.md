---
title: "Collect: Data Capture"
collection: publications
permalink: /publication/CollectDataProfiling
excerpt: 'CDC'
date: 2024-05-01
venue: 'Processing'
tags:
  - Collect
---

For the initial load we would extract all data. This will be a one-time process where all historical data is pulled into the target system. 
If the initial load is too large, break it into smaller batches to avoid overloading the source system.

> Implementation Tip: Very often the inital load is not a simple query on the enitre table because we will hit into size contraints either because of data query tools hit a limit, transfer contraints or downstream systems are not able to read large files. We would need to break these large captures into smaller batches. These batches need to be tested early. Additionally, we need the capability to rebase the entire dataset from the source when issues with the data arise.

However in this page we will discuss the ways to capture source data changes and depending on the scenario we would need to choose the best option.

# Leveraging Timestamp Audit Columns on the database

Modern source applications include timestamp audit columns that store the date and time for every row that is either created or updated. These columns are internally managed. This is the first capability of the source system we need to analyze when designing the component to capture data.

## Analyze Timestamp Audit Columns
Analyze each table to ensure the timestamp audit column (created_at, updated_at, deleted_at) are accurate and reliable. 
There can be issues if these columns are generated by custom implementations.
Verify that all timezone consistency on all the timestamp columns to avoid inconsistencies during extraction.

* In most cases, we find that these tables are updated by a single transaction, which is what is desired. If there is a transaction that updates more than one table, all the tables would have the same timestamp. However, while this level of detail might seem excessive, it is important when defining the cut-off time for the batches.

## Handling Deletes
* If the source system uses soft deletes (marking a row as deleted without physically removing it), ensure the deleted_at column is used to capture these records. This is the least complex in handling deletes.
* If the source system performs hard deletes (physically removing records), we would not have an audit column to indicate deletes, we would need to handle it separately in one of the scenarios below.

