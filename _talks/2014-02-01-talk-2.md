---
title: "Parquet: Standard we were all waiting for"
collection: talks
type: "Talk"
permalink: /talks/Parquet-Storage-Standard
date: 2024-02-01
---

```python
import pandas as pd
import pyarrow as pyar
import pyarrow.parquet as pyarpq
import pyarrow.dataset as pyards
import pyarrow.csv as pyarcsv
```

## Cleaning up the initial schema
To save time creating the schema, we can use the schema which pyarrow automatically generates. 
However it would not be ideal and we would need to redesign the schema based on the best practices.
Though Parquet stores the schema along the with the data, Pyarrow uses a seperate Schema object to manage the schama. This schema can also be seperately saved and managed.
Also as a baseline we will save it as a Parquet and also save the schema as .metadata and redesign the schema


```python
# Read a CSV file into a PyArrow Table and infer the schema
def parse_schema(schema_str):
    type_mapping = {
        "int64": pyar.int64(),
        "string": pyar.string(),
        "float64": pyar.float64(),
        "double": pyar.float64(),
        "null": pyar.string(),
        'binary':pyar.binary()
    }
    fields = []
    for line in schema_str.strip().split('\n'):
        name, type_str = line.split(': ')
        pa_type = type_mapping[type_str]
        fields.append(pyar.field(name, pa_type))
    return pyar.schema(fields)
def parseTables():
    tables = ['Addresses','BusinessPartners','ProductCategories','ProductCategoryText','Products','ProductTexts','SalesOrderItems','SalesOrders','Employees']
    for tableName in tables:
        tablepyar = pyarcsv.read_csv('Data/'+tableName+'.csv')
        csvSchema = tablepyar.schema.to_string();
        print(f'_____________ {tableName}__________________')
        parquet_scheama = parse_schema(csvSchema)
        print(parquet_scheama.to_string().replace('\n',','))
        convert_options= pyarcsv.ConvertOptions(column_types=parquet_scheama)
        custom_csv_format = pyards.CsvFileFormat(convert_options=convert_options)
        dataset = pyards.dataset('Data/'+tableName+'.csv', format=custom_csv_format)
        # Write to Parquet
        table_parquet_table = dataset.to_table()
        pyarpq.write_metadata(table_parquet_table.schema,'Data/'+tableName+'.metadata')
        pyarpq.write_table(table_parquet_table, 'Data/'+tableName+ '.parquet')

parseTables();
```

    _____________ Addresses__________________
    ADDRESSID: int64,CITY: string,POSTALCODE: string,STREET: string,BUILDING: int64,COUNTRY: string,REGION: string,ADDRESSTYPE: int64,VALIDITY_STARTDATE: int64,VALIDITY_ENDDATE: int64,LATITUDE: double,LONGITUDE: double
    _____________ BusinessPartners__________________
    PARTNERID: int64,PARTNERROLE: int64,EMAILADDRESS: string,PHONENUMBER: int64,FAXNUMBER: string,WEBADDRESS: string,ADDRESSID: int64,COMPANYNAME: string,LEGALFORM: string,CREATEDBY: int64,CREATEDAT: int64,CHANGEDBY: int64,CHANGEDAT: int64,CURRENCY: string
    _____________ ProductCategories__________________
    PRODCATEGORYID: string,CREATEDBY: int64,CREATEDAT: int64
    _____________ ProductCategoryText__________________
    PRODCATEGORYID: string,LANGUAGE: string,SHORT_DESCR: string,MEDIUM_DESCR: string,LONG_DESCR: string
    _____________ Products__________________
    PRODUCTID: string,TYPECODE: string,PRODCATEGORYID: string,CREATEDBY: int64,CREATEDAT: int64,CHANGEDBY: int64,CHANGEDAT: int64,SUPPLIER_PARTNERID: int64,TAXTARIFFCODE: int64,QUANTITYUNIT: string,WEIGHTMEASURE: double,WEIGHTUNIT: string,CURRENCY: string,PRICE: int64,WIDTH: string,DEPTH: string,HEIGHT: string,DIMENSIONUNIT: string,PRODUCTPICURL: string
    _____________ ProductTexts__________________
    PRODUCTID: string,LANGUAGE: string,SHORT_DESCR: string,MEDIUM_DESCR: string,LONG_DESCR: string
    _____________ SalesOrderItems__________________
    SALESORDERID: int64,SALESORDERITEM: int64,PRODUCTID: string,NOTEID: string,CURRENCY: string,GROSSAMOUNT: int64,NETAMOUNT: double,TAXAMOUNT: double,ITEMATPSTATUS: string,OPITEMPOS: string,QUANTITY: int64,QUANTITYUNIT: string,DELIVERYDATE: int64
    _____________ SalesOrders__________________
    SALESORDERID: int64,CREATEDBY: int64,CREATEDAT: int64,CHANGEDBY: int64,CHANGEDAT: int64,FISCVARIANT: string,FISCALYEARPERIOD: int64,NOTEID: string,PARTNERID: int64,SALESORG: string,CURRENCY: string,GROSSAMOUNT: int64,NETAMOUNT: double,TAXAMOUNT: double,LIFECYCLESTATUS: string,BILLINGSTATUS: string,DELIVERYSTATUS: string
    _____________ Employees__________________
    EMPLOYEEID: int64,NAME_FIRST: string,NAME_MIDDLE: string,NAME_LAST: string,NAME_INITIALS: string,SEX: string,LANGUAGE: string,PHONENUMBER: string,EMAILADDRESS: string,LOGINNAME: string,ADDRESSID: int64,VALIDITY_STARTDATE: int64,VALIDITY_ENDDATE: int64


We will be focusing on SalesOrders and slowly improve on the schema


```python
df_salesOrder = pd.read_csv('Data/SalesOrders.csv')
schemaString = pyarpq.read_schema("Data/SalesOrders.metadata").to_string().replace('\n',',')
print(f"schema as Sales Order Data  {schemaString}")
df_salesOrder.head(2)
```

    schema as Sales Order Data  SALESORDERID: int64,CREATEDBY: int64,CREATEDAT: int64,CHANGEDBY: int64,CHANGEDAT: int64,FISCVARIANT: string,FISCALYEARPERIOD: int64,NOTEID: string,PARTNERID: int64,SALESORG: string,CURRENCY: string,GROSSAMOUNT: int64,NETAMOUNT: double,TAXAMOUNT: double,LIFECYCLESTATUS: string,BILLINGSTATUS: string,DELIVERYSTATUS: string





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>SALESORDERID</th>
      <th>CREATEDBY</th>
      <th>CREATEDAT</th>
      <th>CHANGEDBY</th>
      <th>CHANGEDAT</th>
      <th>FISCVARIANT</th>
      <th>FISCALYEARPERIOD</th>
      <th>NOTEID</th>
      <th>PARTNERID</th>
      <th>SALESORG</th>
      <th>CURRENCY</th>
      <th>GROSSAMOUNT</th>
      <th>NETAMOUNT</th>
      <th>TAXAMOUNT</th>
      <th>LIFECYCLESTATUS</th>
      <th>BILLINGSTATUS</th>
      <th>DELIVERYSTATUS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>500000000</td>
      <td>4</td>
      <td>20180111</td>
      <td>4</td>
      <td>20180116</td>
      <td>K4</td>
      <td>2018001</td>
      <td>NaN</td>
      <td>100000022</td>
      <td>APJ</td>
      <td>USD</td>
      <td>13587</td>
      <td>11888.625</td>
      <td>1698.375</td>
      <td>C</td>
      <td>C</td>
      <td>C</td>
    </tr>
    <tr>
      <th>1</th>
      <td>500000001</td>
      <td>2</td>
      <td>20180112</td>
      <td>2</td>
      <td>20180115</td>
      <td>K4</td>
      <td>2018001</td>
      <td>NaN</td>
      <td>100000026</td>
      <td>EMEA</td>
      <td>USD</td>
      <td>12622</td>
      <td>11044.250</td>
      <td>1577.750</td>
      <td>C</td>
      <td>C</td>
      <td>C</td>
    </tr>
  </tbody>
</table>
</div>



## Best Practice 1. Use  Appropriate Data Types
Choose the Right Data Types for performance and storage
### 1.1 Date and Time: 
Use timestamp or date types for temporal data. The Sales Order has CREATEDAT and CHANGEDAT as int and we wull update it as date32.

We will use the existing metadata schema file for sales order update it and use it to save the modified dataframe.

Step 1: # using pandas to change the string yymmdd into a date 
Step 2: # get the index and update the schema to date32
Step 3: # Convert the DataFrame to a PyArrow Table using the updated schema and save the new parquet file.


```python
file = pyarpq.ParquetFile('Data/SalesOrders.parquet')
file.schema
```




    <pyarrow._parquet.ParquetSchema object at 0x12642db00>
    required group field_id=-1 schema {
      optional int64 field_id=-1 SALESORDERID;
      optional int64 field_id=-1 CREATEDBY;
      optional int64 field_id=-1 CREATEDAT;
      optional int64 field_id=-1 CHANGEDBY;
      optional int64 field_id=-1 CHANGEDAT;
      optional binary field_id=-1 FISCVARIANT (String);
      optional int64 field_id=-1 FISCALYEARPERIOD;
      optional binary field_id=-1 NOTEID (String);
      optional int64 field_id=-1 PARTNERID;
      optional binary field_id=-1 SALESORG (String);
      optional binary field_id=-1 CURRENCY (String);
      optional int64 field_id=-1 GROSSAMOUNT;
      optional double field_id=-1 NETAMOUNT;
      optional double field_id=-1 TAXAMOUNT;
      optional binary field_id=-1 LIFECYCLESTATUS (String);
      optional binary field_id=-1 BILLINGSTATUS (String);
      optional binary field_id=-1 DELIVERYSTATUS (String);
    }




```python
# using pandas to change the string yymmdd into a date
df_salesOrder['CREATEDAT'] = pd.to_datetime(df_salesOrder['CREATEDAT'], format='%Y%m%d')
df_salesOrder['CHANGEDAT'] = pd.to_datetime(df_salesOrder['CHANGEDAT'], format='%Y%m%d')

# reading the schema from the file
myschema = pyarpq.read_schema("Data/SalesOrders.metadata")
# get the index and update the schema to date32. date64 is for ms which is not required
index = pyar.Schema.get_field_index(myschema, 'CREATEDAT')
myschema = pyar.Schema.set(myschema, index, pyar.field('CREATEDAT', pyar.date32()))
index = pyar.Schema.get_field_index(myschema, 'CHANGEDAT')
myschema = pyar.Schema.set(myschema, index, pyar.field('CHANGEDAT', pyar.date32()))

# Convert the DataFrame to a PyArrow Table using the schema
sales_order_table = pyar.Table.from_pandas(df_salesOrder, schema=myschema)
pyarpq.write_table(
    sales_order_table,
    'Data/SalesOrders.parquet'
)
pyarpq.write_metadata(myschema,'Data/SalesOrders.metadata')
updatedSchema = myschema.to_string().replace('\n',',')
print(f"the new updated schema --> CREATEDAT and CHANGEDAT are updated to date32 format \n {updatedSchema}")
updatedParquetfile = pyarpq.ParquetFile('Data/SalesOrders.parquet')
print('Even the schema on attached to the parquet file is updated.')
updatedParquetfile.schema
```

    the new updated schema --> CREATEDAT and CHANGEDAT are updated to date32 format 
     SALESORDERID: int64,CREATEDBY: int64,CREATEDAT: date32[day],CHANGEDBY: int64,CHANGEDAT: date32[day],FISCVARIANT: string,FISCALYEARPERIOD: int64,NOTEID: string,PARTNERID: int64,SALESORG: string,CURRENCY: string,GROSSAMOUNT: int64,NETAMOUNT: double,TAXAMOUNT: double,LIFECYCLESTATUS: string,BILLINGSTATUS: string,DELIVERYSTATUS: string
    Even the schema on attached to the parquet file is updated.





    <pyarrow._parquet.ParquetSchema object at 0x126996f80>
    required group field_id=-1 schema {
      optional int64 field_id=-1 SALESORDERID;
      optional int64 field_id=-1 CREATEDBY;
      optional int32 field_id=-1 CREATEDAT (Date);
      optional int64 field_id=-1 CHANGEDBY;
      optional int32 field_id=-1 CHANGEDAT (Date);
      optional binary field_id=-1 FISCVARIANT (String);
      optional int64 field_id=-1 FISCALYEARPERIOD;
      optional binary field_id=-1 NOTEID (String);
      optional int64 field_id=-1 PARTNERID;
      optional binary field_id=-1 SALESORG (String);
      optional binary field_id=-1 CURRENCY (String);
      optional int64 field_id=-1 GROSSAMOUNT;
      optional double field_id=-1 NETAMOUNT;
      optional double field_id=-1 TAXAMOUNT;
      optional binary field_id=-1 LIFECYCLESTATUS (String);
      optional binary field_id=-1 BILLINGSTATUS (String);
      optional binary field_id=-1 DELIVERYSTATUS (String);
    }



### 1.2 Avoid Nulls When Possible: 
Design the schema to minimize the use of nulls, as they can affect performance.
NOTEID is null and we will drop the column from the dataframe and also update the schema


```python
# defining this as a function as we will use this a few times.
def updateParquetAndMetaData(df_salesOrder, myschema):
    sales_order_table = pyar.Table.from_pandas(df_salesOrder, schema=myschema)
    pyarpq.write_table(
        sales_order_table,
        'Data/SalesOrders.parquet'
    )
    pyarpq.write_metadata(myschema,'Data/SalesOrders.metadata')
    updatedSchema = myschema.to_string().replace('\n',',')
    print(updatedSchema)
    
# Using Pandas to drop the column 
df_salesOrder = df_salesOrder.drop(['NOTEID'],axis =1)
# removing the column from the meta data too.
index = pyar.Schema.get_field_index(myschema, 'NOTEID')
myschema = pyar.Schema.remove(myschema, index)

print("the new updated schema --> removed the NOTEID \n")
updateParquetAndMetaData(df_salesOrder, myschema)

```

    the new updated schema --> removed the NOTEID 
    
    SALESORDERID: int64,CREATEDBY: int64,CREATEDAT: date32[day],CHANGEDBY: int64,CHANGEDAT: date32[day],FISCVARIANT: string,FISCALYEARPERIOD: int64,PARTNERID: int64,SALESORG: string,CURRENCY: string,GROSSAMOUNT: int64,NETAMOUNT: double,TAXAMOUNT: double,LIFECYCLESTATUS: string,BILLINGSTATUS: string,DELIVERYSTATUS: string


### 1.3 Numerical Data: Use integer or floating-point types to make them more appropriate.
Modifying float64 to float32 as it would suffice for the values we would need.


```python
index = pyar.Schema.get_field_index(myschema, 'TAXAMOUNT')
myschema = pyar.Schema.set(myschema, index, pyar.field('TAXAMOUNT', pyar.float32()))
index = pyar.Schema.get_field_index(myschema, 'NETAMOUNT')
myschema = pyar.Schema.set(myschema, index, pyar.field('NETAMOUNT', pyar.float32()))
index = pyar.Schema.get_field_index(myschema, 'GROSSAMOUNT')
myschema = pyar.Schema.set(myschema, index, pyar.field('GROSSAMOUNT', pyar.int32()))

print(f"the new updated schema -> abover 3 columns would be updated to float from double")
updateParquetAndMetaData(df_salesOrder, myschema)
```

    the new updated schema -> abover 3 columns would be updated to float from double
    SALESORDERID: int64,CREATEDBY: int64,CREATEDAT: date32[day],CHANGEDBY: int64,CHANGEDAT: date32[day],FISCVARIANT: string,FISCALYEARPERIOD: int64,PARTNERID: int64,SALESORG: string,CURRENCY: string,GROSSAMOUNT: int32,NETAMOUNT: float,TAXAMOUNT: float,LIFECYCLESTATUS: string,BILLINGSTATUS: string,DELIVERYSTATUS: string


## Best Practice 2. Leverage Compression and Encoding
Column-Specific Compression: Choose appropriate compression methods for each column based on the data type and characteristics.
Encoding Techniques: Use encoding techniques like dictionary encoding, run-length encoding, or delta encoding to further optimize storage and performance.
Text Data: Use string types, but avoid using strings for numerical or categorical data.

### 2.1 Dictionary Encoding
dictionary<values=string, indices=int8, ordered=0>: This indicates that the category column is stored as a dictionary-encoded column, where the unique values are of type string, and the indices pointing to these values are of type int8.
Benefits of Dictionary Encoding
Storage Efficiency: Only unique values are stored once, reducing storage space.
Query Performance: Lookups are faster as the data is compact and indices are used for comparison.
#### When to use??? Use for columns with low cardinality and high repetition (use_dictionary parameter).

Step 1: Dataframe.astype('category') will automatically convert to catagory 
Step 2:  pyar.field('LIFECYCLESTATUS', pyar.dictionary(pyar.int8(), pyar.utf8())) to define the category



```python
# Convert the 'category' column to a categorical type
df_salesOrder['LIFECYCLESTATUS'] = df_salesOrder['LIFECYCLESTATUS'].astype('category')
df_salesOrder['BILLINGSTATUS'] = df_salesOrder['BILLINGSTATUS'].astype('category')
df_salesOrder['DELIVERYSTATUS'] = df_salesOrder['DELIVERYSTATUS'].astype('category')
df_salesOrder['SALESORG'] = df_salesOrder['SALESORG'].astype('category')

index = pyar.Schema.get_field_index(myschema, 'LIFECYCLESTATUS')
myschema = pyar.Schema.set(myschema, index, pyar.field('LIFECYCLESTATUS', pyar.dictionary(pyar.int8(), pyar.utf8())))
index = pyar.Schema.get_field_index(myschema, 'BILLINGSTATUS')
myschema = pyar.Schema.set(myschema, index, pyar.field('BILLINGSTATUS', pyar.dictionary(pyar.int8(), pyar.utf8())))
index = pyar.Schema.get_field_index(myschema, 'DELIVERYSTATUS')
myschema = pyar.Schema.set(myschema, index, pyar.field('DELIVERYSTATUS', pyar.dictionary(pyar.int8(), pyar.utf8())))
index = pyar.Schema.get_field_index(myschema, 'SALESORG')
myschema = pyar.Schema.set(myschema, index, pyar.field('SALESORG', pyar.dictionary(pyar.int8(), pyar.utf8())))

print(f"We can see the columns are updated to a dictionary")
updateParquetAndMetaData(df_salesOrder, myschema)
```

    We can see the columns are updated to a dictionary
    SALESORDERID: int64,CREATEDBY: int64,CREATEDAT: date32[day],CHANGEDBY: int64,CHANGEDAT: date32[day],FISCVARIANT: string,FISCALYEARPERIOD: int64,PARTNERID: int64,SALESORG: dictionary<values=string, indices=int8, ordered=0>,CURRENCY: string,GROSSAMOUNT: int32,NETAMOUNT: float,TAXAMOUNT: float,LIFECYCLESTATUS: dictionary<values=string, indices=int8, ordered=0>,BILLINGSTATUS: dictionary<values=string, indices=int8, ordered=0>,DELIVERYSTATUS: dictionary<values=string, indices=int8, ordered=0>


### 2.2 Run-Length and Delta Encoding 
RLE is a simple and efficient form of data compression where sequences of the same data value (runs) are stored as a single data value and a count, rather than as the original run. 
This technique is particularly effective for data that contains many consecutive repeated values.
Default encoding is typically enabled which can include RLE for certain data types. However if we want to maintain Dictionary encoding for specific colummns we would need to specify them.
PyArrow automatically applies delta encoding to integer columns where appropriate

Step: # Write the table to a Parquet file with dictionary encoding for 'category' and default encoding for others



```python
# Write the table to a Parquet file with dictionary encoding for 'category' and default encoding for others
pyarpq.write_table(
    pyar.Table.from_pandas(df_salesOrder, preserve_index=False),
    'Data/SalesOrders.parquet',
    use_dictionary=['SALESORG','LIFECYCLESTATUS','BILLINGSTATUS','DELIVERYSTATUS'],  # Specify dictionary encoding for the 'category' column
    compression='SNAPPY',  # Use compression for better storage efficiency
    data_page_version='2.0',  # Use data page version 2.0 for better performance
    write_statistics=True
)
```

## Best Practice 3  Partitioning
Partitioning Strategy: Design your schema to include partition columns that make sense for your queries. Common partition columns are dates, geographic regions, or user IDs.
Balance Partition Size: Ensure that partitions are neither too large nor too small. Aim for a partition size that strikes a balance between query performance and manageability.


```python
df_salesOrder[['FISCALYEAR', 'FISCALMONTH']] = df_salesOrder['FISCALYEARPERIOD'].apply(lambda x: pd.Series([int(str(x)[:4]), int(str(x)[5:])])  )
# Define the partitioning columns based on date1
df_salesOrder = df_salesOrder.drop(['FISCALYEARPERIOD'],axis = 1)
df_salesOrder

```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>SALESORDERID</th>
      <th>CREATEDBY</th>
      <th>CREATEDAT</th>
      <th>CHANGEDBY</th>
      <th>CHANGEDAT</th>
      <th>FISCVARIANT</th>
      <th>PARTNERID</th>
      <th>SALESORG</th>
      <th>CURRENCY</th>
      <th>GROSSAMOUNT</th>
      <th>NETAMOUNT</th>
      <th>TAXAMOUNT</th>
      <th>LIFECYCLESTATUS</th>
      <th>BILLINGSTATUS</th>
      <th>DELIVERYSTATUS</th>
      <th>FISCALYEAR</th>
      <th>FISCALMONTH</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>500000000</td>
      <td>4</td>
      <td>2018-01-11</td>
      <td>4</td>
      <td>2018-01-16</td>
      <td>K4</td>
      <td>100000022</td>
      <td>APJ</td>
      <td>USD</td>
      <td>13587</td>
      <td>11888.625</td>
      <td>1698.375</td>
      <td>C</td>
      <td>C</td>
      <td>C</td>
      <td>2018</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>500000001</td>
      <td>2</td>
      <td>2018-01-12</td>
      <td>2</td>
      <td>2018-01-15</td>
      <td>K4</td>
      <td>100000026</td>
      <td>EMEA</td>
      <td>USD</td>
      <td>12622</td>
      <td>11044.250</td>
      <td>1577.750</td>
      <td>C</td>
      <td>C</td>
      <td>C</td>
      <td>2018</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>500000002</td>
      <td>5</td>
      <td>2018-01-15</td>
      <td>5</td>
      <td>2018-01-20</td>
      <td>K4</td>
      <td>100000018</td>
      <td>APJ</td>
      <td>USD</td>
      <td>45655</td>
      <td>39948.125</td>
      <td>5706.875</td>
      <td>C</td>
      <td>C</td>
      <td>C</td>
      <td>2018</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>500000003</td>
      <td>3</td>
      <td>2018-01-15</td>
      <td>3</td>
      <td>2018-01-20</td>
      <td>K4</td>
      <td>100000009</td>
      <td>EMEA</td>
      <td>USD</td>
      <td>101786</td>
      <td>89062.750</td>
      <td>12723.250</td>
      <td>C</td>
      <td>C</td>
      <td>C</td>
      <td>2018</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>500000004</td>
      <td>8</td>
      <td>2018-01-16</td>
      <td>8</td>
      <td>2018-01-17</td>
      <td>K4</td>
      <td>100000025</td>
      <td>EMEA</td>
      <td>USD</td>
      <td>71684</td>
      <td>62723.500</td>
      <td>8960.500</td>
      <td>C</td>
      <td>C</td>
      <td>C</td>
      <td>2018</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>329</th>
      <td>500000329</td>
      <td>7</td>
      <td>2019-06-19</td>
      <td>7</td>
      <td>2019-06-20</td>
      <td>K4</td>
      <td>100000031</td>
      <td>AMER</td>
      <td>USD</td>
      <td>90621</td>
      <td>79293.375</td>
      <td>11327.625</td>
      <td>I</td>
      <td>I</td>
      <td>C</td>
      <td>2019</td>
      <td>6</td>
    </tr>
    <tr>
      <th>330</th>
      <td>500000330</td>
      <td>11</td>
      <td>2019-06-22</td>
      <td>11</td>
      <td>2019-06-26</td>
      <td>K4</td>
      <td>100000000</td>
      <td>AMER</td>
      <td>USD</td>
      <td>113871</td>
      <td>99637.125</td>
      <td>14233.875</td>
      <td>I</td>
      <td>I</td>
      <td>C</td>
      <td>2019</td>
      <td>6</td>
    </tr>
    <tr>
      <th>331</th>
      <td>500000331</td>
      <td>3</td>
      <td>2019-06-25</td>
      <td>3</td>
      <td>2019-06-26</td>
      <td>K4</td>
      <td>100000009</td>
      <td>EMEA</td>
      <td>USD</td>
      <td>12693</td>
      <td>11106.375</td>
      <td>1586.625</td>
      <td>I</td>
      <td>I</td>
      <td>I</td>
      <td>2019</td>
      <td>6</td>
    </tr>
    <tr>
      <th>332</th>
      <td>500000332</td>
      <td>13</td>
      <td>2019-06-27</td>
      <td>13</td>
      <td>2019-07-01</td>
      <td>K4</td>
      <td>100000034</td>
      <td>AMER</td>
      <td>USD</td>
      <td>70528</td>
      <td>61712.000</td>
      <td>8816.000</td>
      <td>I</td>
      <td>I</td>
      <td>C</td>
      <td>2019</td>
      <td>6</td>
    </tr>
    <tr>
      <th>333</th>
      <td>500000333</td>
      <td>5</td>
      <td>2019-06-27</td>
      <td>5</td>
      <td>2019-07-01</td>
      <td>K4</td>
      <td>100000022</td>
      <td>APJ</td>
      <td>USD</td>
      <td>16576</td>
      <td>14504.000</td>
      <td>2072.000</td>
      <td>I</td>
      <td>I</td>
      <td>I</td>
      <td>2019</td>
      <td>6</td>
    </tr>
  </tbody>
</table>
<p>334 rows Ã— 17 columns</p>
</div>




```python
table_SalesOrder = pyar.Table.from_pandas(df_salesOrder)
partition_cols = ['FISCALYEAR', 'FISCALMONTH']
# Write the table to Parquet partitioned by 'partition_year' and 'partition_month'
# Write the table to a Parquet file with partitioning

# removing the column from the meta data too.
index = pyar.Schema.get_field_index(myschema, 'FISCALYEARPERIOD')
myschema = pyar.Schema.remove(myschema, index)
myschema = pyar.Schema.append(myschema, pyar.field('FISCALYEAR',pyar.int32()))
myschema = pyar.Schema.append(myschema, pyar.field('FISCALMONTH',pyar.int32()))
pyarpq.write_to_dataset(
    table_SalesOrder,
    root_path='SalerOrderPartition',
    partition_cols= partition_cols,
    use_dictionary=['SALESORG','LIFECYCLESTATUS','BILLINGSTATUS','DELIVERYSTATUS'],  # Specify dictionary encoding for the 'category' column
    schema = myschema
    compression='snappy'  # Optional: Use compression for storage efficiency
)

```


```python
# Define the path to the partitioned dataset
dataset_path = 'SalerOrderPartition/'

# Create a dataset object
dataset = pyards.dataset(dataset_path, format='parquet', partitioning='hive')

# Convert to a table and then to a pandas DataFrame
table = dataset.to_table()
print(table.schema)
df = table.to_pandas()

# Display the DataFrame
df.head(3)
```

    SALESORDERID: int64
    CREATEDBY: int64
    CREATEDAT: timestamp[ns]
    CHANGEDBY: int64
    CHANGEDAT: timestamp[ns]
    FISCVARIANT: string
    PARTNERID: int64
    SALESORG: dictionary<values=string, indices=int32, ordered=0>
    CURRENCY: string
    GROSSAMOUNT: int64
    NETAMOUNT: double
    TAXAMOUNT: double
    LIFECYCLESTATUS: dictionary<values=string, indices=int32, ordered=0>
    BILLINGSTATUS: dictionary<values=string, indices=int32, ordered=0>
    DELIVERYSTATUS: dictionary<values=string, indices=int32, ordered=0>
    FISCALYEAR: int32
    FISCALMONTH: int32
    -- schema metadata --
    pandas: '{"index_columns": [{"kind": "range", "name": null, "start": 0, "' + 2460





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>SALESORDERID</th>
      <th>CREATEDBY</th>
      <th>CREATEDAT</th>
      <th>CHANGEDBY</th>
      <th>CHANGEDAT</th>
      <th>FISCVARIANT</th>
      <th>PARTNERID</th>
      <th>SALESORG</th>
      <th>CURRENCY</th>
      <th>GROSSAMOUNT</th>
      <th>NETAMOUNT</th>
      <th>TAXAMOUNT</th>
      <th>LIFECYCLESTATUS</th>
      <th>BILLINGSTATUS</th>
      <th>DELIVERYSTATUS</th>
      <th>FISCALYEAR</th>
      <th>FISCALMONTH</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>500000000</td>
      <td>4</td>
      <td>2018-01-11</td>
      <td>4</td>
      <td>2018-01-16</td>
      <td>K4</td>
      <td>100000022</td>
      <td>APJ</td>
      <td>USD</td>
      <td>13587</td>
      <td>11888.625</td>
      <td>1698.375</td>
      <td>C</td>
      <td>C</td>
      <td>C</td>
      <td>2018</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>500000001</td>
      <td>2</td>
      <td>2018-01-12</td>
      <td>2</td>
      <td>2018-01-15</td>
      <td>K4</td>
      <td>100000026</td>
      <td>EMEA</td>
      <td>USD</td>
      <td>12622</td>
      <td>11044.250</td>
      <td>1577.750</td>
      <td>C</td>
      <td>C</td>
      <td>C</td>
      <td>2018</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>500000002</td>
      <td>5</td>
      <td>2018-01-15</td>
      <td>5</td>
      <td>2018-01-20</td>
      <td>K4</td>
      <td>100000018</td>
      <td>APJ</td>
      <td>USD</td>
      <td>45655</td>
      <td>39948.125</td>
      <td>5706.875</td>
      <td>C</td>
      <td>C</td>
      <td>C</td>
      <td>2018</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




```python
# Convert the DataFrame to a PyArrow Table
# Write the table to a Parquet file
# Convert the PyArrow Table to a Pandas DataFrame
table = pyar.Table.from_pandas(df, preserve_index=False)
strschema = table.schema.to_string();
print(strschema)

```

1. Understand Your Data and Queries
Analyze Data Characteristics: Understand the types of data you are working with, including data types, data distribution, and frequency of updates.
Identify Query Patterns: Analyze the types of queries you will run most frequently. Identify which columns are often filtered, aggregated, or joined.

3. Columnar Storage Benefits
Optimize for Columnar Storage: Since Parquet is a columnar storage format, design your schema to take advantage of this. Group columns that are frequently queried together.
Reduce Redundancy: Avoid storing redundant data. Normalize your schema to reduce duplication, but balance it with the need for efficient querying.
4. Nested Data Structures
Use Nested Structures: Parquet supports complex nested data structures (e.g., maps, arrays, structs). Use them to represent hierarchical data more efficiently.
Flatten When Necessary: If nested structures complicate querying, consider flattening the schema.


# 1. Schema Design




## Normalized Data Makes Life Easy
Avoid excessive nesting. Deeply nested structures can be harder to query and may impact performance. Normalize your data where possible.
### When to Nest Data: 
1. When columns are always accessed together, nesting can make sense.
2. Read-heavy where performance if impacted because of the need for joins.
3. Self-Contained Data
### When to Normalize:
1. For nested structures, use struct, list, or map types where appropriate.
1. If nesting leads to significant data redundancy (e.g., repeating user information for every order)
2. When down stream systems need to process data. ETL pipelines work better with normalized tables
3. When nested data have relationships with other data entities which are independently managed or queried.




# Schema Evolution 

6. Schema Evolution
Plan for Changes: Design your schema to be flexible for future changes. Parquet supports schema evolution, allowing you to add or modify columns without breaking existing queries.
Backward and Forward Compatibility: Ensure that changes to the schema maintain compatibility with existing data and applications.


Schema evolution in Parquet refers to the capability of the Parquet file format to handle changes in the schema (structure) of the data over time without breaking compatibility with older data. This feature allows you to modify the schema, such as adding new columns or changing data types, while still being able to read and write data using the updated schema.

Key Aspects of Schema Evolution in Parquet
Adding Columns: You can add new columns to the schema. When reading data, the new columns will have null or default values for the old data that does not contain these columns.

Removing Columns: You can remove columns from the schema. When reading older data, the removed columns will be ignored.

Changing Data Types: Parquet supports certain types of data type changes (e.g., widening types like int to long). However, not all type changes are supported, and some may require more careful handling or data migration.

Reordering Columns: Parquet does not require columns to be in a specific order, so reordering columns in the schema does not affect data compatibility.

Benefits of Schema Evolution
Flexibility: Allows the schema to evolve as the data requirements change over time without needing to rewrite all the historical data.
Backward Compatibility: Ensures that older data can still be read with new schema versions.
Forward Compatibility: Ensures that new data written with an updated schema can still be read by systems expecting the old schema.

