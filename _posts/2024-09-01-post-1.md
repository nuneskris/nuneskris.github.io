---
title: 'Hello PyTorch'
date: 2024-09-14
permalink: /posts/2024/09/01-post-1/
tags:
  - cool posts
  - category1
  - category2
---

The objective is to use the very basic example of linear regression and use PyTorch to build a model to demonstrate Pytorch workflow and its fundementals.

I will develop a simple model using Pytorch and they use a layered NN to perform the same prediction.

```python
import torch
from torch import nn
import matplotlib.pyplot as plt
torch.__version__
```

    '2.4.1+cu121'

## Simple linear regression problem
I will use the freecodecamp example which I find very usefull to get into the fundementals. This will try to build a model which trains 2 variables; Bias and weight of a line. We will generate sample data with predefined values of .3 and .7 and train the model based on the sample data which we have generated.

### generate sample data


```python
# We will generate data based on the predefined weight and bias. this will generate a sample set of 5 values
weight = .7
bias = .3
start = 0
end = 1
step = .02
X = torch.arange(start, end, step).unsqueeze(dim=1)
print(X.shape)
y = weight * X + bias
X[:5] , y[:5]
```

    torch.Size([50, 1])





    (tensor([[0.0000],
             [0.0200],
             [0.0400],
             [0.0600],
             [0.0800]]),
     tensor([[0.3000],
             [0.3140],
             [0.3280],
             [0.3420],
             [0.3560]]))




```python
# prompt: split the data X and y for train and test at random positions

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"X_train shape: {X_train.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_test shape: {y_test.shape}")
```

    X_train shape: torch.Size([40, 1])
    y_train shape: torch.Size([40, 1])
    X_test shape: torch.Size([10, 1])
    y_test shape: torch.Size([10, 1])


### Visualize the sample data


```python
# prompt: visualize the X_train, X_test, y_train, y_test

plt.figure(figsize=(10, 7))
plt.scatter(X_train, y_train, c="b", s=4, label="Training data")
plt.scatter(X_test, y_test, c="r", s=4, label="Testing data")
plt.legend(fontsize=14)
plt.xlabel("X")
plt.ylabel("y")
plt.show()
```

<img width="612" alt="image" src="/images/posts/DeepLearningStudyICan/DeepLearningStudyICan_7_0.png">

## Model Development

So we need to build a model close to the above line and test the model on the red dots and get a result as close to it.

This is developed by inheriting the nn.Module class

***Step 1*** : initialize the paramters of bias and weight to a random value

***Step 2*** : Build a model using a custom funcion = X * weight + Bias.




```python
class LinearRegressionModelKFN(nn.Module):
    # The objective is to start with a random weight and bias and then usen the pytorch to figure out the best value based on the training data.
    # This is done either by gradient decent or Back propogation.
    # we have set requires_grad=True

    def __init__(self):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))
        self.bias = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))

    # we need to overide the forward method
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.weight * x + self.bias
```

###  Initial random values of our model


```python
torch.manual_seed(42)
model_0 = LinearRegressionModelKFN()
print(list(model_0.parameters()))
print(model_0.state_dict())
with torch.inference_mode():
    y_preds = model_0(X_test)

y_preds
```

    [Parameter containing:
    tensor([0.3367], requires_grad=True), Parameter containing:
    tensor([0.1288], requires_grad=True)]
    OrderedDict([('weight', tensor([0.3367])), ('bias', tensor([0.1288]))])





    tensor([[0.2163],
            [0.3914],
            [0.3308],
            [0.4318],
            [0.2433],
            [0.4520],
            [0.3039],
            [0.2972],
            [0.3443],
            [0.2568]])



### Ploting the initial model with random parameters

We need to use pytorch to bring the green line as close to the red line


```python
# prompt: add to the plt the y_preds

plt.figure(figsize=(10, 7))
plt.scatter(X_train, y_train, c="b", s=4, label="Training data")
plt.scatter(X_test, y_test, c="r", s=4, label="Testing data")
plt.scatter(X_test, y_preds, c="g",  s=4, label="Predictions")
plt.legend(fontsize=14)
plt.xlabel("X")
plt.ylabel("y")
plt.show()

```
<img width="612" alt="image" src="/images/posts/DeepLearningStudyICan/DeepLearningStudyICan_13_0.png">

## Pytorch Workflow

We will define a loss function and a optimizer and iteratively use them to enhance the model by reducing the loss function.

We will also track the progress of the loss function


```python
# prompt: set a lossfunction using L1Loss and a Optimizer using SGD
loss_fn = nn.L1Loss()
optimizer = torch.optim.SGD(params = model_0.parameters(), lr=0.001)

# variable as used to iterate the number of time.
epochs = 1500

# Below variables are used to track the progress
epoch_values = []
loss_values = []
test_loss_values = []

# Workflow
for epoch in range(epochs):
    # set the model into training mode
    model_0.train()

    # Step 1: Forward Pass
    y_preds = model_0(X_train)

    # Step 2: Calculate the loss
    loss = loss_fn(y_preds, y_train)

    # Step 3: Opimizer zero grad to reset every loop
    optimizer.zero_grad()

    # Step 4: Perform back propogation
    loss.backward()

    # Step 5: Step the optimizer
    optimizer.step()


    # Progress tracking
    if epoch % 100 == 0:
      print(f"Epoch: {epoch} | Loss: {loss}")
      model_0.eval()
      print(model_0.state_dict())
      with torch.inference_mode():
        test_preds = model_0(X_test)
        test_loss = loss_fn(test_preds, y_test)
      epoch_values.append(epoch)
      loss_values.append(loss.detach().numpy())
      test_loss_values.append(test_loss.detach().numpy())

```

    Epoch: 0 | Loss: 0.340311199426651
    OrderedDict([('weight', tensor([0.3372])), ('bias', tensor([0.1298]))])
    Epoch: 100 | Loss: 0.21864144504070282
    OrderedDict([('weight', tensor([0.3837])), ('bias', tensor([0.2298]))])
    Epoch: 200 | Loss: 0.10451823472976685
    OrderedDict([('weight', tensor([0.4301])), ('bias', tensor([0.3256]))])
    Epoch: 300 | Loss: 0.06448396295309067
    OrderedDict([('weight', tensor([0.4709])), ('bias', tensor([0.3713]))])
    Epoch: 400 | Loss: 0.05163732171058655
    OrderedDict([('weight', tensor([0.5030])), ('bias', tensor([0.3857]))])
    Epoch: 500 | Loss: 0.045265451073646545
    OrderedDict([('weight', tensor([0.5278])), ('bias', tensor([0.3827]))])
    Epoch: 600 | Loss: 0.03966418653726578
    OrderedDict([('weight', tensor([0.5492])), ('bias', tensor([0.3727]))])
    Epoch: 700 | Loss: 0.03406292945146561
    OrderedDict([('weight', tensor([0.5707])), ('bias', tensor([0.3627]))])
    Epoch: 800 | Loss: 0.028461668640375137
    OrderedDict([('weight', tensor([0.5921])), ('bias', tensor([0.3527]))])
    Epoch: 900 | Loss: 0.02286040410399437
    OrderedDict([('weight', tensor([0.6136])), ('bias', tensor([0.3427]))])
    Epoch: 1000 | Loss: 0.0172591470181942
    OrderedDict([('weight', tensor([0.6351])), ('bias', tensor([0.3327]))])
    Epoch: 1100 | Loss: 0.011657887138426304
    OrderedDict([('weight', tensor([0.6565])), ('bias', tensor([0.3227]))])
    Epoch: 1200 | Loss: 0.006066909525543451
    OrderedDict([('weight', tensor([0.6776])), ('bias', tensor([0.3121]))])
    Epoch: 1300 | Loss: 0.0004846327065024525
    OrderedDict([('weight', tensor([0.6984])), ('bias', tensor([0.3009]))])
    Epoch: 1400 | Loss: 0.00019141807570122182
    OrderedDict([('weight', tensor([0.7002])), ('bias', tensor([0.3009]))])


### Plotting the progress tracking


```python
# prompt: Plot to compare epoch_values with loss_values and test_loss_values

plt.figure(figsize=(10, 7))
plt.plot(epoch_values, loss_values, label="Training Loss")
plt.plot(epoch_values, test_loss_values, label="Test Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training and Test Loss vs Epoch")
plt.legend()
plt.show()

```

<img width="612" alt="image" src="/images/posts/DeepLearningStudyICan/DeepLearningStudyICan_17_0.png">

Plotting the model output predictions


```python
# prompt: plot the new y_preds along with the Xtrain and X test
model_0.eval()
print(model_0.state_dict())
with torch.inference_mode():
    y_preds = model_0(X_test)

plt.figure(figsize=(10, 7))
plt.scatter(X_train, y_train, c="b", s=4, label="Training data")
plt.scatter(X_test, y_test, c="r", s=20, label="Testing data")
plt.scatter(X_test, y_preds, c="g",  s=4, label="Predictions")
plt.legend(fontsize=14)
plt.xlabel("X")
plt.ylabel("y")
plt.show()

```

    OrderedDict([('weight', tensor([0.6998])), ('bias', tensor([0.2999]))])

<img width="612" alt="image" src="/images/posts/DeepLearningStudyICan/DeepLearningStudyICan_19_1.png">

# 2 layered NN

```python
# prompt: Create a new model which will perform the same but using a 2 layered nueral network


# Define a two-layer linear regression model
class LinearRegressionModelKFN2Layer(nn.Module):
    def __init__(self):
        """
        Constructor for the LinearRegressionModelKFN2Layer class. This initializes a simple neural 
        network with two linear layers. The first linear layer transforms the input with one feature 
        into a hidden layer with 10 neurons. The second linear layer transforms the hidden layer's 
        output into a single value (the predicted output).
        """
        super().__init__()
        
        # Define the first linear layer (input layer), which takes 1 feature and outputs 10 neurons
        self.linear1 = nn.Linear(1, 10)
        
        # Define the second linear layer (output layer), which takes 10 neurons as input and outputs 1 value
        self.linear2 = nn.Linear(10, 1)

    def forward(self, x):
        """
        Forward pass through the neural network. This method takes the input `x`, applies the first
        linear transformation, passes it through the ReLU activation function, and then applies the
        second linear transformation to get the output.

        Args:
        x (Tensor): Input tensor containing the feature(s), where each row is an example and 
                    each column is a feature (for this case, there is 1 feature per example).

        Returns:
        Tensor: The output of the model, which is the predicted value.
        """
        # Apply the first linear layer, followed by a ReLU activation function
        x = torch.relu(self.linear1(x))
        
        # Apply the second linear layer to get the final output (predicted value)
        x = self.linear2(x)
        
        return x


torch.manual_seed(42)
model_2layer = LinearRegressionModelKFN2Layer()
print(list(model_2layer.parameters()))
print(model_2layer.state_dict())
with torch.inference_mode():
    y_preds = model_2layer(X_test)

y_preds

# ### Ploting the initial model with random parameters
# 
# We need to use pytorch to bring the green line as close to the red line

plt.figure(figsize=(10, 7))
plt.scatter(X_train, y_train, c="b", s=4, label="Training data")
plt.scatter(X_test, y_test, c="r", s=4, label="Testing data")
plt.scatter(X_test, y_preds, c="g",  s=4, label="Predictions")
plt.legend(fontsize=14)
plt.xlabel("X")
plt.ylabel("y")
plt.show()

# ## Pytorch Workflow
# 
# We will define a loss function and a optimizer and iteratively use them to enhance the model by reducing the loss function.
# 
# We will also track the progress of the loss function
loss_fn = nn.L1Loss()
optimizer = torch.optim.SGD(params = model_2layer.parameters(), lr=0.001)

# variable as used to iterate the number of time.
epochs = 1500

# Below variables are used to track the progress
epoch_values = []
loss_values = []
test_loss_values = []

# Workflow
for epoch in range(epochs):
    # set the model into training mode
    model_2layer.train()

    # Step 1: Forward Pass
    y_preds = model_2layer(X_train)

    # Step 2: Calculate the loss
    loss = loss_fn(y_preds, y_train)

    # Step 3: Opimizer zero grad to reset every loop
    optimizer.zero_grad()

    # Step 4: Perform back propogation
    loss.backward()

    # Step 5: Step the optimizer
    optimizer.step()


    # Progress tracking
    if epoch % 100 == 0:
      print(f"Epoch: {epoch} | Loss: {loss}")
      model_2layer.eval()
      print(model_2layer.state_dict())
      with torch.inference_mode():
        test_preds = model_2layer(X_test)
        test_loss = loss_fn(test_preds, y_test)
      epoch_values.append(epoch)
      loss_values.append(loss.detach().numpy())
      test_loss_values.append(test_loss.detach().numpy())

# ### Plotting the progress tracking

plt.figure(figsize=(10, 7))
plt.plot(epoch_values, loss_values, label="Training Loss")
plt.plot(epoch_values, test_loss_values, label="Test Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training and Test Loss vs Epoch")
plt.legend()
plt.show()

# Plotting the model output predictions
model_2layer.eval()
print(model_2layer.state_dict())
with torch.inference_mode():
    y_preds = model_2layer(X_test)

plt.figure(figsize=(10, 7))
plt.scatter(X_train, y_train, c="b", s=4, label="Training data")
plt.scatter(X_test, y_test, c="r", s=20, label="Testing data")
plt.scatter(X_test, y_preds, c="g",  s=4, label="Predictions")
plt.legend(fontsize=14)
plt.xlabel("X")
plt.ylabel("y")
plt.show()
```


    [Parameter containing:
    tensor([[ 0.7645],
            [ 0.8300],
            [-0.2343],
            [ 0.9186],
            [-0.2191],
            [ 0.2018],
            [-0.4869],
            [ 0.5873],
            [ 0.8815],
            [-0.7336]], requires_grad=True), Parameter containing:
    tensor([ 0.8692,  0.1872,  0.7388,  0.1354,  0.4822, -0.1412,  0.7709,  0.1478,
            -0.4668,  0.2549], requires_grad=True), Parameter containing:
    tensor([[-0.1457, -0.0371, -0.1284,  0.2098, -0.2496, -0.1458, -0.0893, -0.1901,
              0.0298, -0.3123]], requires_grad=True), Parameter containing:
    tensor([0.2856], requires_grad=True)]
    OrderedDict([('linear1.weight', tensor([[ 0.7645],
            [ 0.8300],
            [-0.2343],
            [ 0.9186],
            [-0.2191],
            [ 0.2018],
            [-0.4869],
            [ 0.5873],
            [ 0.8815],
            [-0.7336]])), ('linear1.bias', tensor([ 0.8692,  0.1872,  0.7388,  0.1354,  0.4822, -0.1412,  0.7709,  0.1478,
            -0.4668,  0.2549])), ('linear2.weight', tensor([[-0.1457, -0.0371, -0.1284,  0.2098, -0.2496, -0.1458, -0.0893, -0.1901,
              0.0298, -0.3123]])), ('linear2.bias', tensor([0.2856]))])
